{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f65892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "#from langchain.chains.quetion_answering import load_qa_chain\n",
    "#from langchain.chains import load_qa_chain\n",
    "#from langchain.chains import QAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa63ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-stack\n",
      "  Obtaining dependency information for llama-stack from https://files.pythonhosted.org/packages/0f/c2/acf46de50c846b86e09491d55baf0c16ba8ca342692f878ff85cf073e5e3/llama_stack-0.2.12-py3-none-any.whl.metadata\n",
      "  Downloading llama_stack-0.2.12-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (3.11.11)\n",
      "Collecting fastapi<1.0,>=0.115.0 (from llama-stack)\n",
      "  Obtaining dependency information for fastapi<1.0,>=0.115.0 from https://files.pythonhosted.org/packages/e5/47/d63c60f59a59467fda0f93f46335c9d18526d7071f025cb5b89d5353ea42/fastapi-0.116.1-py3-none-any.whl.metadata\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting fire (from llama-stack)\n",
      "  Obtaining dependency information for fire from https://files.pythonhosted.org/packages/e5/4c/93d0f85318da65923e4b91c1c2ff03d8a458cbefebe3bc612a6693c7906d/fire-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: httpx in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (0.27.1)\n",
      "Collecting jinja2>=3.1.6 (from llama-stack)\n",
      "  Obtaining dependency information for jinja2>=3.1.6 from https://files.pythonhosted.org/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl.metadata\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: jsonschema in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (4.23.0)\n",
      "Collecting llama-stack-client>=0.2.12 (from llama-stack)\n",
      "  Obtaining dependency information for llama-stack-client>=0.2.12 from https://files.pythonhosted.org/packages/68/eb/23c10e3a7d7a49a316e05ddcca8cd712a5a474eb6f904d08f7d3ae10d668/llama_stack_client-0.2.12-py3-none-any.whl.metadata\n",
      "  Downloading llama_stack_client-0.2.12-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting openai>=1.66 (from llama-stack)\n",
      "  Obtaining dependency information for openai>=1.66 from https://files.pythonhosted.org/packages/00/e1/47887212baa7bc0532880d33d5eafbdb46fcc4b53789b903282a74a85b5b/openai-1.106.1-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.106.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: prompt-toolkit in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (3.0.48)\n",
      "Requirement already satisfied: python-dotenv in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (1.0.1)\n",
      "Collecting python-jose (from llama-stack)\n",
      "  Obtaining dependency information for python-jose from https://files.pythonhosted.org/packages/d9/c3/0bd11992072e6a1c513b16500a5d07f91a24017c5909b02c72c62d7ad024/python_jose-3.5.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading python_jose-3.5.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (2.10.5)\n",
      "Requirement already satisfied: requests in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (2.32.3)\n",
      "Requirement already satisfied: rich in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (13.9.4)\n",
      "Requirement already satisfied: setuptools in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (65.5.0)\n",
      "Collecting starlette (from llama-stack)\n",
      "  Obtaining dependency information for starlette from https://files.pythonhosted.org/packages/ce/fd/901cfa59aaa5b30a99e16876f11abe38b59a1a2c51ffb3d7142bb6089069/starlette-0.47.3-py3-none-any.whl.metadata\n",
      "  Downloading starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting termcolor (from llama-stack)\n",
      "  Obtaining dependency information for termcolor from https://files.pythonhosted.org/packages/4f/bd/de8d508070629b6d84a30d01d57e4a65c69aa7f5abe7560b8fad3b50ea59/termcolor-3.1.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: tiktoken in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (0.8.0)\n",
      "Requirement already satisfied: pillow in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (11.1.0)\n",
      "Collecting h11>=0.16.0 (from llama-stack)\n",
      "  Obtaining dependency information for h11>=0.16.0 from https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting python-multipart>=0.0.20 (from llama-stack)\n",
      "  Obtaining dependency information for python-multipart>=0.0.20 from https://files.pythonhosted.org/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl.metadata\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from fastapi<1.0,>=0.115.0->llama-stack) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from jinja2>=3.1.6->llama-stack) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (4.8.0)\n",
      "Requirement already satisfied: click in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (8.1.8)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (1.9.0)\n",
      "Requirement already satisfied: pandas in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (2.2.3)\n",
      "Collecting pyaml (from llama-stack-client>=0.2.12->llama-stack)\n",
      "  Obtaining dependency information for pyaml from https://files.pythonhosted.org/packages/a8/ee/a878f2ad010cbccb311f947f0f2f09d38f613938ee28c34e60fceecc75a1/pyaml-25.7.0-py3-none-any.whl.metadata\n",
      "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sniffio in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (1.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (4.67.1)\n",
      "Requirement already satisfied: certifi in c:\\windows\\system32\\llm_env\\lib\\site-packages (from httpx->llama-stack) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\windows\\system32\\llm_env\\lib\\site-packages (from httpx->llama-stack) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\windows\\system32\\llm_env\\lib\\site-packages (from httpx->llama-stack) (3.10)\n",
      "INFO: pip is looking at multiple versions of httpcore to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httpcore==1.* (from httpx->llama-stack)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from openai>=1.66->llama-stack) (0.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pydantic>=2->llama-stack) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pydantic>=2->llama-stack) (2.27.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (1.18.3)\n",
      "Requirement already satisfied: filelock in c:\\windows\\system32\\llm_env\\lib\\site-packages (from huggingface-hub->llama-stack) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from huggingface-hub->llama-stack) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from huggingface-hub->llama-stack) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from huggingface-hub->llama-stack) (6.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from jsonschema->llama-stack) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from jsonschema->llama-stack) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from jsonschema->llama-stack) (0.22.3)\n",
      "Requirement already satisfied: wcwidth in c:\\windows\\system32\\llm_env\\lib\\site-packages (from prompt-toolkit->llama-stack) (0.2.13)\n",
      "Collecting ecdsa!=0.15 (from python-jose->llama-stack)\n",
      "  Obtaining dependency information for ecdsa!=0.15 from https://files.pythonhosted.org/packages/cb/a3/460c57f094a4a165c84a1341c373b0a4f5ec6ac244b998d5021aade89b77/ecdsa-0.19.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading ecdsa-0.19.1-py2.py3-none-any.whl.metadata (29 kB)\n",
      "Collecting rsa!=4.1.1,!=4.4,<5.0,>=4.0 (from python-jose->llama-stack)\n",
      "  Obtaining dependency information for rsa!=4.1.1,!=4.4,<5.0,>=4.0 from https://files.pythonhosted.org/packages/64/8d/0133e4eb4beed9e425d9a98ed6e081a55d195481b7632472be1af08d2f6b/rsa-4.9.1-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.5.0 (from python-jose->llama-stack)\n",
      "  Obtaining dependency information for pyasn1>=0.5.0 from https://files.pythonhosted.org/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from requests->llama-stack) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from requests->llama-stack) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from rich->llama-stack) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from rich->llama-stack) (2.19.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from tiktoken->llama-stack) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from ecdsa!=0.15->python-jose->llama-stack) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.2)\n",
      "Requirement already satisfied: colorama in c:\\windows\\system32\\llm_env\\lib\\site-packages (from tqdm->llama-stack-client>=0.2.12->llama-stack) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2024.2)\n",
      "Downloading llama_stack-0.2.12-py3-none-any.whl (3.7 MB)\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/3.7 MB 5.3 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.3/3.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.5/3.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.7/3.7 MB 4.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.9/3.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.1/3.7 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.3/3.7 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.5/3.7 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.7/3.7 MB 4.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.9/3.7 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.4/3.7 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.6/3.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.8/3.7 MB 4.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.1/3.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.2/3.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.5/3.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.7/3.7 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "   ---------------------------------------- 0.0/95.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 95.6/95.6 kB 5.3 MB/s eta 0:00:00\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.9/134.9 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading llama_stack_client-0.2.12-py3-none-any.whl (340 kB)\n",
      "   ---------------------------------------- 0.0/340.2 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 286.7/340.2 kB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 340.2/340.2 kB 5.3 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.8/78.8 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading openai-1.106.1-py3-none-any.whl (930 kB)\n",
      "   ---------------------------------------- 0.0/930.8 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 225.3/930.8 kB 6.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 450.6/930.8 kB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 573.4/930.8 kB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/930.8 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 930.8/930.8 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading starlette-0.47.3-py3-none-any.whl (72 kB)\n",
      "   ---------------------------------------- 0.0/73.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 73.0/73.0 kB ? eta 0:00:00\n",
      "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.9/115.9 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading python_jose-3.5.0-py2.py3-none-any.whl (34 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading ecdsa-0.19.1-py2.py3-none-any.whl (150 kB)\n",
      "   ---------------------------------------- 0.0/150.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 150.6/150.6 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "   ---------------------------------------- 0.0/83.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 83.1/83.1 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: termcolor, python-multipart, pyasn1, pyaml, jinja2, h11, ecdsa, starlette, rsa, httpcore, fire, python-jose, fastapi, openai, llama-stack-client, llama-stack\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Windows\\\\System32\\\\LLM_env\\\\Lib\\\\site-packages\\\\termcolor'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: C:\\Windows\\System32\\LLM_env\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install llama-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97848bf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1500980386.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    llama model list\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "llama model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0320486",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (485951401.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    llama model list --show-all\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "llama model list --show-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "363f5da4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2619164901.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    llama model download --source meta --model-id  MODEL_ID\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "llama model download --source meta --model-id  MODEL_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d842bca",
   "metadata": {},
   "source": [
    "# STEP - 1 Create Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd61df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PdfReader(\"cpa 2019 [bare text].pdf\")\n",
    "text = ''\n",
    "cnt = 0\n",
    "for page in pdf_reader.pages:\n",
    "    text = text + page.extract_text()\n",
    "    cnt+=1\n",
    "    if cnt>30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780354d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97920"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "818750b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer Protection Act, \n",
      "2019\n"
     ]
    }
   ],
   "source": [
    "print(text[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff6d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200,length_function=len)\n",
    "chunks = text_splitter.split_text(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364f927c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a4b054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer Protection Act, \n",
      "2019  \n",
      "&  \n",
      "1. Corrigendum to CP Act 2019  \n",
      "2. Effective date of Consumer Protection Act, 2019  \n",
      "3. The Consumer Protection (Consumer Disputes \n",
      "Redressal Commissions) Rules, 2020  \n",
      "4. The Consumer Protection (Central Consumer \n",
      "Protection Council) Rules, 2020  \n",
      "5. The Consumer Protection (Qualification for \n",
      "appointment, method of recruitment, procedure of \n",
      "appointment, term of office, resignation and \n",
      "removal of the President and members of State \n",
      "Commission and District Commission) Rules, 2020   \n",
      "6. The Consume r Protection (Salary, allowances and \n",
      "conditions of service of President and Members of \n",
      "the State Commission and District Commission) \n",
      "Model Rules, 2020.  \n",
      "7. The Consumer Protection (Mediation) Rules, 2020  \n",
      " \n",
      " \n",
      "July 17, 2020  Short title,\n",
      "extent,commencementandapplication.THE CONSUMER PROTECTION ACT, 2019\n",
      "NO. 35 OF  2019\n",
      "[9th August , 2019.]\n",
      "An Act to provide for protection of the interests of consumers and for the said\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca153c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANMATI\\AppData\\Local\\Temp\\ipykernel_21012\\459717908.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc9874",
   "metadata": {},
   "source": [
    "# Use LLaMA as the Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "655051ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-stack\n",
      "  Downloading llama_stack-0.2.12-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack) (3.8.5)\n",
      "Collecting fastapi<1.0,>=0.115.0 (from llama-stack)\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting fire (from llama-stack)\n",
      "  Using cached fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: httpx in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack) (0.15.1)\n",
      "Collecting jinja2>=3.1.6 (from llama-stack)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack) (4.17.3)\n",
      "Collecting llama-stack-client>=0.2.12 (from llama-stack)\n",
      "  Downloading llama_stack_client-0.2.12-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting openai>=1.66 (from llama-stack)\n",
      "  Downloading openai-1.106.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: prompt-toolkit in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack) (3.0.36)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack) (0.21.0)\n",
      "Collecting python-jose (from llama-stack)\n",
      "  Using cached python_jose-3.5.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting pydantic>=2 (from llama-stack)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "     ---------------------------------------- 0.0/68.0 kB ? eta -:--:--\n",
      "     ------------------------ --------------- 41.0/68.0 kB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 61.4/68.0 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 68.0/68.0 kB 461.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack) (2.31.0)\n",
      "Requirement already satisfied: rich in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack) (13.7.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack) (68.0.0)\n",
      "Requirement already satisfied: starlette in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack) (0.37.2)\n",
      "Collecting termcolor (from llama-stack)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting tiktoken (from llama-stack)\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack) (9.3.0)\n",
      "Collecting h11>=0.16.0 (from llama-stack)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting python-multipart>=0.0.20 (from llama-stack)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting starlette (from llama-stack)\n",
      "  Using cached starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from fastapi<1.0,>=0.115.0->llama-stack) (4.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from jinja2>=3.1.6->llama-stack) (2.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (8.0.4)\n",
      "Collecting distro<2,>=1.7.0 (from llama-stack-client>=0.2.12->llama-stack)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (2.2.3)\n",
      "Collecting pyaml (from llama-stack-client>=0.2.12->llama-stack)\n",
      "  Using cached pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (4.65.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from httpx->llama-stack) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from httpx->llama-stack) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from httpx->llama-stack) (3.4)\n",
      "INFO: pip is looking at multiple versions of httpcore to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httpcore==1.* (from httpx->llama-stack)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.66->llama-stack)\n",
      "  Downloading jiter-0.10.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2->llama-stack)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2->llama-stack)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from fastapi<1.0,>=0.115.0->llama-stack)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2->llama-stack)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from llama-stack-client>=0.2.12->llama-stack)\n",
      "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from aiohttp->llama-stack) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from aiohttp->llama-stack) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from aiohttp->llama-stack) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from aiohttp->llama-stack) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from aiohttp->llama-stack) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from aiohttp->llama-stack) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from aiohttp->llama-stack) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from huggingface-hub->llama-stack) (3.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from huggingface-hub->llama-stack) (2023.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from huggingface-hub->llama-stack) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from huggingface-hub->llama-stack) (23.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from jsonschema->llama-stack) (0.18.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from prompt-toolkit->llama-stack) (0.2.5)\n",
      "Collecting ecdsa!=0.15 (from python-jose->llama-stack)\n",
      "  Using cached ecdsa-0.19.1-py2.py3-none-any.whl.metadata (29 kB)\n",
      "Collecting rsa!=4.1.1,!=4.4,<5.0,>=4.0 (from python-jose->llama-stack)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.5.0 (from python-jose->llama-stack)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from requests->llama-stack) (1.26.16)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from rich->llama-stack) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from rich->llama-stack) (2.15.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from tiktoken->llama-stack) (2022.7.9)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from ecdsa!=0.15->python-jose->llama-stack) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from tqdm->llama-stack-client>=0.2.12->llama-stack) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2023.3)\n",
      "Downloading llama_stack-0.2.12-py3-none-any.whl (3.7 MB)\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.7 MB 62.2 kB/s eta 0:00:59\n",
      "   ---------------------------------------- 0.0/3.7 MB 62.2 kB/s eta 0:00:59\n",
      "   ---------------------------------------- 0.0/3.7 MB 72.8 kB/s eta 0:00:50\n",
      "    --------------------------------------- 0.1/3.7 MB 109.2 kB/s eta 0:00:33\n",
      "    --------------------------------------- 0.1/3.7 MB 119.1 kB/s eta 0:00:31\n",
      "   - -------------------------------------- 0.1/3.7 MB 236.4 kB/s eta 0:00:15\n",
      "   -- ------------------------------------- 0.2/3.7 MB 360.4 kB/s eta 0:00:10\n",
      "   --- ------------------------------------ 0.3/3.7 MB 441.9 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 0.5/3.7 MB 625.5 kB/s eta 0:00:06\n",
      "   ------ --------------------------------- 0.6/3.7 MB 737.3 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.7/3.7 MB 818.7 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.8/3.7 MB 904.9 kB/s eta 0:00:04\n",
      "   --------- ------------------------------ 0.9/3.7 MB 1.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.9/3.7 MB 1.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.9/3.7 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.0/3.7 MB 932.7 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.0/3.7 MB 987.8 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.3/3.7 MB 701.1 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 1.3/3.7 MB 705.4 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 1.4/3.7 MB 720.9 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 1.5/3.7 MB 783.2 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.7/3.7 MB 847.7 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 1.8/3.7 MB 909.2 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.0/3.7 MB 972.9 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/0f/c2/acf46de50c846b86e09491d55baf0c16ba8ca342692f878ff85cf073e5e3/llama_stack-0.2.12-py3-none-any.whl\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/0f/c2/acf46de50c846b86e09491d55baf0c16ba8ca342692f878ff85cf073e5e3/llama_stack-0.2.12-py3-none-any.whl\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.3 requires pydantic<2.0, but you have pydantic 2.11.7 which is incompatible.\n",
      "jupyter-server 1.23.4 requires anyio<4,>=3.1.0, but you have anyio 4.10.0 which is incompatible.\n",
      "pyasn1-modules 0.2.8 requires pyasn1<0.5.0,>=0.4.6, but you have pyasn1 0.6.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.7 MB 992.7 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 2.4/3.7 MB 261.9 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 2.4/3.7 MB 267.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 2.5/3.7 MB 279.1 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 2.7/3.7 MB 299.0 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 2.9/3.7 MB 318.1 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 3.1/3.7 MB 336.6 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 3.3/3.7 MB 352.6 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 3.4/3.7 MB 366.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.7 MB 387.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.7 MB 389.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.6/3.7 MB 389.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.7/3.7 MB 386.2 kB/s eta 0:00:00\n",
      "Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading llama_stack_client-0.2.12-py3-none-any.whl (340 kB)\n",
      "   ---------------------------------------- 0.0/340.2 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 276.5/340.2 kB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  337.9/340.2 kB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 340.2/340.2 kB 2.6 MB/s eta 0:00:00\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading openai-1.106.1-py3-none-any.whl (930 kB)\n",
      "   ---------------------------------------- 0.0/930.8 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 122.9/930.8 kB 2.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 256.0/930.8 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 430.1/930.8 kB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 655.4/930.8 kB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 839.7/930.8 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/930.8 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  921.6/930.8 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 930.8/930.8 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "   ---------------------------------------- 0.0/444.8 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 286.7/444.8 kB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  440.3/444.8 kB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  440.3/444.8 kB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 444.8/444.8 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.0 MB 5.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.0 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.9/2.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.2/2.0 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/2.0 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 4.8 MB/s eta 0:00:00\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached starlette-0.47.3-py3-none-any.whl (72 kB)\n",
      "Using cached fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "Using cached python_jose-3.5.0-py2.py3-none-any.whl (34 kB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl (884 kB)\n",
      "   ---------------------------------------- 0.0/884.4 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 256.0/884.4 kB 7.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 481.3/884.4 kB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 716.8/884.4 kB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  880.6/884.4 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 884.4/884.4 kB 4.7 MB/s eta 0:00:00\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.2 kB ? eta -:--:--\n",
      "   -------------------------------------- - 102.4/107.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 107.2/107.2 kB 2.1 MB/s eta 0:00:00\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached ecdsa-0.19.1-py2.py3-none-any.whl (150 kB)\n",
      "Downloading jiter-0.10.0-cp311-cp311-win_amd64.whl (209 kB)\n",
      "   ---------------------------------------- 0.0/209.2 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 153.6/209.2 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  204.8/209.2 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 209.2/209.2 kB 1.6 MB/s eta 0:00:00\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.6 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 41.0/44.6 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 41.0/44.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.6/44.6 kB 369.6 kB/s eta 0:00:00\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: typing-extensions, termcolor, python-multipart, pyasn1, pyaml, jiter, jinja2, h11, ecdsa, distro, annotated-types, typing-inspection, tiktoken, rsa, pydantic-core, httpcore, fire, anyio, starlette, python-jose, pydantic, openai, llama-stack-client, fastapi, llama-stack\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: python-multipart\n",
      "    Found existing installation: python-multipart 0.0.9\n",
      "    Uninstalling python-multipart-0.0.9:\n",
      "      Successfully uninstalled python-multipart-0.0.9\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.4.8\n",
      "    Uninstalling pyasn1-0.4.8:\n",
      "      Successfully uninstalled pyasn1-0.4.8\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.2\n",
      "    Uninstalling Jinja2-3.1.2:\n",
      "      Successfully uninstalled Jinja2-3.1.2\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.5\n",
      "    Uninstalling httpcore-1.0.5:\n",
      "      Successfully uninstalled httpcore-1.0.5\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.5.0\n",
      "    Uninstalling anyio-3.5.0:\n",
      "      Successfully uninstalled anyio-3.5.0\n",
      "  Attempting uninstall: starlette\n",
      "    Found existing installation: starlette 0.37.2\n",
      "    Uninstalling starlette-0.37.2:\n",
      "      Successfully uninstalled starlette-0.37.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.8\n",
      "    Uninstalling pydantic-1.10.8:\n",
      "      Successfully uninstalled pydantic-1.10.8\n",
      "  Attempting uninstall: fastapi\n",
      "    Found existing installation: fastapi 0.111.0\n",
      "    Uninstalling fastapi-0.111.0:\n",
      "      Successfully uninstalled fastapi-0.111.0\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.10.0 distro-1.9.0 ecdsa-0.19.1 fastapi-0.116.1 fire-0.7.1 h11-0.16.0 httpcore-1.0.9 jinja2-3.1.6 jiter-0.10.0 llama-stack-0.2.12 llama-stack-client-0.2.12 openai-1.106.1 pyaml-25.7.0 pyasn1-0.6.1 pydantic-2.11.7 pydantic-core-2.33.2 python-jose-3.5.0 python-multipart-0.0.20 rsa-4.9.1 starlette-0.47.3 termcolor-3.1.0 tiktoken-0.11.0 typing-extensions-4.15.0 typing-inspection-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade llama-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "333cabfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\sanmati\\anaconda3\\lib\\site-packages (24.0)\n",
      "Collecting pip\n",
      "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\SANMATI\\anaconda3\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Downloading numpy-2.3.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.9 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/60.9 kB 108.9 kB/s eta 0:00:01\n",
      "     ------------------- ------------------ 30.7/60.9 kB 186.2 kB/s eta 0:00:01\n",
      "     ------------------- ------------------ 30.7/60.9 kB 186.2 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/60.9 kB 217.9 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.9/60.9 kB 179.8 kB/s eta 0:00:00\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading numpy-2.3.2-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "   ---------------------------------------- 0.0/13.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/13.1 MB 3.0 MB/s eta 0:00:05\n",
      "   ---------------------------------------- 0.1/13.1 MB 3.0 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.2/13.1 MB 1.6 MB/s eta 0:00:09\n",
      "    --------------------------------------- 0.2/13.1 MB 1.6 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.5/13.1 MB 2.2 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.5/13.1 MB 2.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.9/13.1 MB 2.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.2/13.1 MB 3.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.5/13.1 MB 3.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.8/13.1 MB 4.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/13.1 MB 4.2 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.4/13.1 MB 4.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.7/13.1 MB 4.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.0/13.1 MB 4.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.4/13.1 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.7/13.1 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.1/13.1 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.5/13.1 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.8/13.1 MB 5.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.2/13.1 MB 5.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.2/13.1 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.9/13.1 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.1/13.1 MB 5.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.6/13.1 MB 5.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 7.0/13.1 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 7.2/13.1 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.2/13.1 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.2/13.1 MB 6.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.2/13.1 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.8/13.1 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.2/13.1 MB 6.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.6/13.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.0/13.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.4/13.1 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.8/13.1 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.2/13.1 MB 7.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.5/13.1 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.0/13.1 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.3/13.1 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.7/13.1 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.1 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.1 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.1 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.1/13.1 MB 7.2 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [20 lines of output]\n",
      "  \u001b[32m*** \u001b[1mscikit-build-core 0.11.6\u001b[0m using \u001b[34mCMake 4.1.0\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "  \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  2025-09-07 21:59:55,714 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "  loading initial cache file C:\\Users\\SANMATI\\AppData\\Local\\Temp\\tmpexulhnkb\\build\\CMakeInit.txt\n",
      "  -- Building for: NMake Makefiles\n",
      "  \u001b[31mCMake Error at CMakeLists.txt:3 (project):\n",
      "    Running\n",
      "  \n",
      "     'nmake' '-?'\n",
      "  \n",
      "    failed with:\n",
      "  \n",
      "     no such file or directory\n",
      "  \n",
      "  \u001b[0m\n",
      "  \u001b[0mCMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\u001b[0m\n",
      "  \u001b[0mCMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\u001b[0m\n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  \u001b[31m\n",
      "  \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade --force-reinstall llama-cpp-python --prefer-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a3aad10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\sanmati\\llm_env_new\\lib\\site-packages (from llama-cpp-python) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\sanmati\\llm_env_new\\lib\\site-packages (from llama-cpp-python) (2.3.2)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\sanmati\\llm_env_new\\lib\\site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sanmati\\llm_env_new\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build llama-cpp-python\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [20 lines of output]\n",
      "  \u001b[32m*** \u001b[1mscikit-build-core 0.11.6\u001b[0m using \u001b[34mCMake 4.1.0\u001b[39m\u001b[0m \u001b[31m(wheel)\u001b[0m\n",
      "  \u001b[32m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  2025-09-07 21:59:53,266 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None\n",
      "  loading initial cache file C:\\Users\\SANMATI\\AppData\\Local\\Temp\\tmpli8n3klf\\build\\CMakeInit.txt\n",
      "  -- Building for: NMake Makefiles\n",
      "  \u001b[31mCMake Error at CMakeLists.txt:3 (project):\n",
      "    Running\n",
      "  \n",
      "     'nmake' '-?'\n",
      "  \n",
      "    failed with:\n",
      "  \n",
      "     no such file or directory\n",
      "  \n",
      "  \u001b[0m\n",
      "  \u001b[0mCMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\u001b[0m\n",
      "  \u001b[0mCMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\u001b[0m\n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  \u001b[31m\n",
      "  \u001b[1m***\u001b[0m \u001b[31mCMake configuration failed\u001b[0m\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for llama-cpp-python\n",
      "ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\SANMATI\\LLM_env_new\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68c630bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: llama-cpp-python\n"
     ]
    }
   ],
   "source": [
    "!pip show llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57893377",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_cpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[32m      3\u001b[39m llm = Llama(model_path=\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mSANMATI\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mRAG-CPA\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mllama-3.2-3B\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mggml-model.bin\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m output = llm(\u001b[33m\"\u001b[39m\u001b[33mSummarize the Consumer Protection Act in 3 lines.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=r\"C:\\Users\\SANMATI\\RAG-CPA\\models\\llama-3.2-3B\\ggml-model.bin\")\n",
    "\n",
    "output = llm(\"Summarize the Consumer Protection Act in 3 lines.\")\n",
    "print(output)\n",
    "\n",
    "prompt = \"Summarize the Consumer Protection Act in 3 lines.\"\n",
    "\n",
    "response = llama.generate(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f72a90f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\System32\\LLM_env\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f72bbdd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2774045778.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[24], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"C:\\Windows\\System32\\LLM_env\\Scripts\\python.exe\" -m pip install --upgrade llama-stack\u001b[0m\n\u001b[1;37m                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"C:\\Windows\\System32\\LLM_env\\Scripts\\python.exe\" -m pip install --upgrade llama-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ee9ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-stack\n",
      "  Obtaining dependency information for llama-stack from https://files.pythonhosted.org/packages/0f/c2/acf46de50c846b86e09491d55baf0c16ba8ca342692f878ff85cf073e5e3/llama_stack-0.2.12-py3-none-any.whl.metadata\n",
      "  Using cached llama_stack-0.2.12-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (3.11.11)\n",
      "Collecting fastapi<1.0,>=0.115.0 (from llama-stack)\n",
      "  Obtaining dependency information for fastapi<1.0,>=0.115.0 from https://files.pythonhosted.org/packages/e5/47/d63c60f59a59467fda0f93f46335c9d18526d7071f025cb5b89d5353ea42/fastapi-0.116.1-py3-none-any.whl.metadata\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting fire (from llama-stack)\n",
      "  Obtaining dependency information for fire from https://files.pythonhosted.org/packages/e5/4c/93d0f85318da65923e4b91c1c2ff03d8a458cbefebe3bc612a6693c7906d/fire-0.7.1-py3-none-any.whl.metadata\n",
      "  Using cached fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: httpx in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (0.27.1)\n",
      "Collecting jinja2>=3.1.6 (from llama-stack)\n",
      "  Obtaining dependency information for jinja2>=3.1.6 from https://files.pythonhosted.org/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl.metadata\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: jsonschema in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (4.23.0)\n",
      "Collecting llama-stack-client>=0.2.12 (from llama-stack)\n",
      "  Obtaining dependency information for llama-stack-client>=0.2.12 from https://files.pythonhosted.org/packages/68/eb/23c10e3a7d7a49a316e05ddcca8cd712a5a474eb6f904d08f7d3ae10d668/llama_stack_client-0.2.12-py3-none-any.whl.metadata\n",
      "  Using cached llama_stack_client-0.2.12-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting openai>=1.66 (from llama-stack)\n",
      "  Obtaining dependency information for openai>=1.66 from https://files.pythonhosted.org/packages/00/e1/47887212baa7bc0532880d33d5eafbdb46fcc4b53789b903282a74a85b5b/openai-1.106.1-py3-none-any.whl.metadata\n",
      "  Using cached openai-1.106.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: prompt-toolkit in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (3.0.48)\n",
      "Requirement already satisfied: python-dotenv in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (1.0.1)\n",
      "Collecting python-jose (from llama-stack)\n",
      "  Obtaining dependency information for python-jose from https://files.pythonhosted.org/packages/d9/c3/0bd11992072e6a1c513b16500a5d07f91a24017c5909b02c72c62d7ad024/python_jose-3.5.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached python_jose-3.5.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (2.10.5)\n",
      "Requirement already satisfied: requests in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (2.32.3)\n",
      "Requirement already satisfied: rich in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (13.9.4)\n",
      "Requirement already satisfied: setuptools in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (65.5.0)\n",
      "Collecting starlette (from llama-stack)\n",
      "  Obtaining dependency information for starlette from https://files.pythonhosted.org/packages/ce/fd/901cfa59aaa5b30a99e16876f11abe38b59a1a2c51ffb3d7142bb6089069/starlette-0.47.3-py3-none-any.whl.metadata\n",
      "  Using cached starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting termcolor (from llama-stack)\n",
      "  Obtaining dependency information for termcolor from https://files.pythonhosted.org/packages/4f/bd/de8d508070629b6d84a30d01d57e4a65c69aa7f5abe7560b8fad3b50ea59/termcolor-3.1.0-py3-none-any.whl.metadata\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: tiktoken in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (0.8.0)\n",
      "Requirement already satisfied: pillow in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack) (11.1.0)\n",
      "Collecting h11>=0.16.0 (from llama-stack)\n",
      "  Obtaining dependency information for h11>=0.16.0 from https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl.metadata\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting python-multipart>=0.0.20 (from llama-stack)\n",
      "  Obtaining dependency information for python-multipart>=0.0.20 from https://files.pythonhosted.org/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl.metadata\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from fastapi<1.0,>=0.115.0->llama-stack) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from jinja2>=3.1.6->llama-stack) (3.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (4.8.0)\n",
      "Requirement already satisfied: click in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (8.1.8)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (1.9.0)\n",
      "Requirement already satisfied: pandas in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (2.2.3)\n",
      "Collecting pyaml (from llama-stack-client>=0.2.12->llama-stack)\n",
      "  Obtaining dependency information for pyaml from https://files.pythonhosted.org/packages/a8/ee/a878f2ad010cbccb311f947f0f2f09d38f613938ee28c34e60fceecc75a1/pyaml-25.7.0-py3-none-any.whl.metadata\n",
      "  Using cached pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: sniffio in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (1.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\windows\\system32\\llm_env\\lib\\site-packages (from llama-stack-client>=0.2.12->llama-stack) (4.67.1)\n",
      "Requirement already satisfied: certifi in c:\\windows\\system32\\llm_env\\lib\\site-packages (from httpx->llama-stack) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\windows\\system32\\llm_env\\lib\\site-packages (from httpx->llama-stack) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\windows\\system32\\llm_env\\lib\\site-packages (from httpx->llama-stack) (3.10)\n",
      "INFO: pip is looking at multiple versions of httpcore to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httpcore==1.* (from httpx->llama-stack)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl.metadata\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from openai>=1.66->llama-stack) (0.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pydantic>=2->llama-stack) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pydantic>=2->llama-stack) (2.27.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from aiohttp->llama-stack) (1.18.3)\n",
      "Requirement already satisfied: filelock in c:\\windows\\system32\\llm_env\\lib\\site-packages (from huggingface-hub->llama-stack) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from huggingface-hub->llama-stack) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from huggingface-hub->llama-stack) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from huggingface-hub->llama-stack) (6.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from jsonschema->llama-stack) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from jsonschema->llama-stack) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from jsonschema->llama-stack) (0.22.3)\n",
      "Requirement already satisfied: wcwidth in c:\\windows\\system32\\llm_env\\lib\\site-packages (from prompt-toolkit->llama-stack) (0.2.13)\n",
      "Collecting ecdsa!=0.15 (from python-jose->llama-stack)\n",
      "  Obtaining dependency information for ecdsa!=0.15 from https://files.pythonhosted.org/packages/cb/a3/460c57f094a4a165c84a1341c373b0a4f5ec6ac244b998d5021aade89b77/ecdsa-0.19.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached ecdsa-0.19.1-py2.py3-none-any.whl.metadata (29 kB)\n",
      "Collecting rsa!=4.1.1,!=4.4,<5.0,>=4.0 (from python-jose->llama-stack)\n",
      "  Obtaining dependency information for rsa!=4.1.1,!=4.4,<5.0,>=4.0 from https://files.pythonhosted.org/packages/64/8d/0133e4eb4beed9e425d9a98ed6e081a55d195481b7632472be1af08d2f6b/rsa-4.9.1-py3-none-any.whl.metadata\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.5.0 (from python-jose->llama-stack)\n",
      "  Obtaining dependency information for pyasn1>=0.5.0 from https://files.pythonhosted.org/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl.metadata\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from requests->llama-stack) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from requests->llama-stack) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from rich->llama-stack) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from rich->llama-stack) (2.19.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from tiktoken->llama-stack) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from ecdsa!=0.15->python-jose->llama-stack) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->llama-stack) (0.1.2)\n",
      "Requirement already satisfied: colorama in c:\\windows\\system32\\llm_env\\lib\\site-packages (from tqdm->llama-stack-client>=0.2.12->llama-stack) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\windows\\system32\\llm_env\\lib\\site-packages (from pandas->llama-stack-client>=0.2.12->llama-stack) (2024.2)\n",
      "Using cached llama_stack-0.2.12-py3-none-any.whl (3.7 MB)\n",
      "Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached llama_stack_client-0.2.12-py3-none-any.whl (340 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached openai-1.106.1-py3-none-any.whl (930 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached starlette-0.47.3-py3-none-any.whl (72 kB)\n",
      "Using cached fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "Using cached python_jose-3.5.0-py2.py3-none-any.whl (34 kB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached ecdsa-0.19.1-py2.py3-none-any.whl (150 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: termcolor, python-multipart, pyasn1, pyaml, jinja2, h11, ecdsa, starlette, rsa, httpcore, fire, python-jose, fastapi, openai, llama-stack-client, llama-stack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Windows\\\\System32\\\\LLM_env\\\\Lib\\\\site-packages\\\\termcolor'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: C:\\Windows\\System32\\LLM_env\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade llama-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e93e1ad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_stack'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_stack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_stack is ready!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_stack'"
     ]
    }
   ],
   "source": [
    "from llama_stack import Llama\n",
    "print(\"llama_stack is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057370cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_env_new)",
   "language": "python",
   "name": "llm_env_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
